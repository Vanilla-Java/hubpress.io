<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>Batching and Low Latency</title>
    <meta name="description" content="" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="https://vanilla-java.github.io/favicon.ico">

    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:400,700,400italic,700italic|Open+Sans:400italic,700italic,700,400">
    <link rel="stylesheet" type="text/css" href="//vanilla-java.github.io/themes/roon/assets/css/screen.css?v=1536077092548" />

    <link rel="canonical" href="https://vanilla-java.github.io/2016/07/09/Batching-and-Low-Latency.html" />
    <meta name="referrer" content="origin" />
    
    <meta property="og:site_name" content="Vanilla Java" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Batching and Low Latency" />
    <meta property="og:description" content="This is testing the next release of Chronicle Queue 4.5.0 Why batch your data? Batching of multiple messages or updates into a single transaction is a common technique for improving performance, in particular it is useful for increasing throughput (messages per second) when the latency (time to process" />
    <meta property="og:url" content="https://vanilla-java.github.io/2016/07/09/Batching-and-Low-Latency.html" />
    <meta property="article:published_time" content="2016-07-09T00:00:00.000Z" />
    <meta property="article:tag" content="Microservices" />
    <meta property="article:tag" content="Batching" />
    <meta property="article:tag" content="Low Latency" />
    
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Batching and Low Latency" />
    <meta name="twitter:description" content="This is testing the next release of Chronicle Queue 4.5.0 Why batch your data? Batching of multiple messages or updates into a single transaction is a common technique for improving performance, in particular it is useful for increasing throughput (messages per second) when the latency (time to process" />
    <meta name="twitter:url" content="https://vanilla-java.github.io/2016/07/09/Batching-and-Low-Latency.html" />
    
    <script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Article",
    "publisher": "Vanilla Java",
    "author": {
        "@type": "Person",
        "name": "Peter Lawrey",
        "image": "https://avatars0.githubusercontent.com/u/1070321?v=4",
        "url": "https://vanilla-java.github.io/author/peter-lawrey/",
        "sameAs": "http://vanillajava.blogspot.com/",
        "description": "Most answers for Java and JVM on StackOverflow.com (~13K), &quot;Vanilla Java&quot; blog with four million views, founder of the Performance JUG,  Java Champion"
    },
    "headline": "Batching and Low Latency",
    "url": "https://vanilla-java.github.io/2016/07/09/Batching-and-Low-Latency.html",
    "datePublished": "2016-07-09T00:00:00.000Z",
    "keywords": "Microservices, Batching, Low Latency",
    "description": "This is testing the next release of Chronicle Queue 4.5.0 Why batch your data? Batching of multiple messages or updates into a single transaction is a common technique for improving performance, in particular it is useful for increasing throughput (messages per second) when the latency (time to process"
}
    </script>

    <meta name="generator" content="HubPress" />
    <link rel="alternate" type="application/rss+xml" title="Vanilla Java" href="https://vanilla-java.github.io/rss/" />
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/themes/prism-okaidia.min.css">
    
        <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
</head>
<body class="post-template tag-Microservices tag-Batching tag-Low-Latency  noimage">

    


    <article role="main" class="">
        <header>
            <a href="https://vanilla-java.github.io" id="home_link">Â«</a>
            <div class="meta"><time datetime="2016-07-09"><a href="/">July 09, 2016</a></time> <span class="count" id="js-reading-time"></span></div>
            <h1>Batching and Low Latency</h1>
        </header>

        <div class="text" id="js-post-content">
            <div id="preamble">
<div class="sectionbody">
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This is testing the next release of Chronicle Queue 4.5.0
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_why_batch_your_data">Why batch your data?</h3>
<div class="paragraph">
<p>Batching of multiple messages or updates into a single transaction is a common technique for improving performance, in particular it is useful for increasing throughput (messages per second) when the latency (time to process a single message) is high.  However when latencies are low, can batching still help?</p>
</div>
<div class="paragraph">
<p>A couple of weeks ago, I was looking at testing and optimising our Chronicle Queue replication over TCP.  One of the first tests I did was to see how batching changed the performance and it made a dramatic difference, and I assumed this was a bug, batching shouldn&#8217;t help a well tuned low latency system.  Now I have had a chance to optimise replication, does batching make much difference?</p>
</div>
</div>
<div class="sect2">
<h3 id="_why_do_batches_help_hinder">Why do batches help/hinder?</h3>
<div class="paragraph">
<p>When your system has an operation with a significant latency, you can utilise more of the available bandwidth by increasing the batch size.</p>
</div>
<div class="paragraph">
<p>Imagine you have a questionaire with 10 questions you want to ask someone. You could send an email for each question and wait for the response to each question before asking the next one. Say it takes 1 minute to answer each question but 10 minutes to send an email, have the person notice it and for you to notice the response.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>sending one question at a time, means each question takes 10 + 1 minutes or 110 minutes in total.</p>
</li>
<li>
<p>sending all 10 questions at once means that the whole questionaire takes 10 + 1 * 10 minutes, or 20 minutes in total.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>It makes sense to ask all your questions at once and get the replies in one go.</p>
</div>
<div class="paragraph">
<p>However, there is a problem, what if you</p>
</div>
<div class="ulist">
<ul>
<li>
<p>don&#8217;t know all the questions in advance because you are still thinking of them. You have to wait to think of the questions, and you might not know how long this will take.</p>
</li>
<li>
<p>might need to ask each question in response to a previous answer.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The longer you wait to build a batch, the more you delay the processing of the messages you have.</p>
</div>
<div class="paragraph">
<p>Batching works best when you know how many messages you are about to get or you know what delay would be acceptable and you can delay sending the messages or committing the transaction for that amount of time.</p>
</div>
<div class="paragraph">
<p>What if you want to process each message as soon as possible?</p>
</div>
</div>
<div class="sect2">
<h3 id="_what_is_being_tested">What is being tested?</h3>
<div class="paragraph">
<p>I am testing the latency under a fixed throughput for publishing a small 40 byte message including time to</p>
</div>
<div class="ulist">
<ul>
<li>
<p>write the message to the persisted queue.</p>
</li>
<li>
<p>read the message and write over TCP.</p>
</li>
<li>
<p>read the TCP socket and write it to a copy of the queue</p>
</li>
<li>
<p>read the message from the queue copy.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To correct for co-ordinated omission, the sending timestamp used is the time the message should have been sent, indead of the time it was actually sent. For the received time, I used <code>System.nanoTime()</code> to get high resolution timing. The replication is over loopback for a consistent nanoTime(). As I am interested in the outliers, I am assuming the overhead using a low latency 10 GigE network wouldn&#8217;t impact these results significantly.  The highest throughput test here would use less than 20% of a 10 GigE network connection as each message is small.</p>
</div>
<div class="paragraph">
<p>The test spends 30 seconds warming up, and 120 seconds running.  The distribution of latencies for each message is summarised.</p>
</div>
<div class="paragraph">
<p>As I have discussed in prevous posts, I am focusing on minimising the 99%ile latency (worst 1 in 100).  On the system, I am reporting no more than 100 million messages per second as this is close to the point there the 99% starts to increase dramatically. i.e. it is close the limit of what this system can process in soft real time.</p>
</div>
<div class="paragraph">
<p>A Centos 7 machine with i7-4790 and 32 GB of memory was used.</p>
</div>
</div>
<div class="sect2">
<h3 id="_end_to_end_latency_for_different_batch_sizes">End to End latency for different batch sizes</h3>
<table class="tableblock frame-all grid-all spread">
<caption class="title">Table 1. 99%ile latency end to end</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Batch Size</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">10 million events per minute</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">60 million events per minute</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">100 million events per minute</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">20 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">28 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><span class="red">176 &micro;s</span></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">22 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">29 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">30 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">5</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">38 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">22 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">27 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">10</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">72 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">26 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">27 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">20</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">125 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">34 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">31 &micro;s</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
1 &micro;s is 0.001 ms or 1 millionth of a second.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>So you can see that batching helps for size of one to five 40 byte messages when the delay between messages is small. However when the delay between messages is relatively large, in this case, every 6 &micro;s, having to wait for enough messages to build a batch hurts the latency.</p>
</div>
<div class="paragraph">
<p>The high latency for the single message at a time for 100 Mmsg/s may yet prove to be a performance bug which can be solved later, but for now, batching could help reduce the jitter, but you don&#8217;t want any more batching so you need to avoid any impact.  In this case, I would consider a batch of two, rather than a larger batch size.</p>
</div>
</div>
<div class="sect2">
<h3 id="_why_not_look_at_the_typical_or_average_latency">Why not look at the typical or average latency?</h3>
<div class="paragraph">
<p>The typical/average latencies are usually better which is why vendors like to report them.  However, they only tell you what the performance looks like momentarily when everything is going well.  By looking at the 99%ile or 99.9%ile you are looking at how something performs when things are not going well.</p>
</div>
<div class="paragraph">
<p>By comparison, the typical latencies all look fine, and you would not know that batching might help at 100 messages per minute.</p>
</div>
<table class="tableblock frame-all grid-all spread">
<caption class="title">Table 2. 50%ile latency end to end</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Batch Size</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">10 million events per minute</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">60 million events per minute</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">100 million events per minute</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">10 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">13 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">16 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">13 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">13 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">13 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">5</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">22 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">13 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">13 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">10</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">36 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">15 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">13 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">20</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">68 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">20 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">15 &micro;s</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Having to wait for a whole batch adds latency and this can be far higher than the time you save. You want your batching to be no larger than necessary.</p>
</div>
</div>
<div class="sect2">
<h3 id="_how_is_batching_used_internally">How is batching used internally?</h3>
<div class="paragraph">
<p>While batching messages published made a little difference in the example above, we use batching internally because it can make a big difference when passing lots of small messages over TCP.</p>
</div>
<div class="paragraph">
<p>As the overhead of passing data over TCP is relatively high, it makes sense to group available data up to some threshold such as 4 KB.  When reading the queue you can determine whether more data is pending as you read it.</p>
</div>
<div class="paragraph">
<p>In particular, making a socket write can be 5 - 10 &micro;s. However we are writing a message around a micro-second or less, this would add an enormous amount to every message if they were sent individually.  By having a background thread batching up these messages we can reduce the latency impact of the socket write.</p>
</div>
</div>
<div class="sect2">
<h3 id="_conclusion">Conclusion</h3>
<div class="paragraph">
<p>Batching can be helpful in reducing the impact of latency to improve throughput.  When you are reaching the limit of the throughput you can achieve, batching can improve the overhead and give you higher throughputs and lower latencies.</p>
</div>
<div class="paragraph">
<p>However, batching is not always possible or appropriate and you want a solution which has low latencies even when you are not reaching the limits of your throughput.</p>
</div>
</div>
<div class="sect2">
<h3 id="_a_quick_look_at_the_99_9_ile">A quick look at the 99.9%ile.</h3>
<div class="paragraph">
<p>These worst 1 in 1000 has mixed result which needs further investigation.  I suspect, the OS might be a cause of this jitter. Note: we didn&#8217;t use thread pinning and that might have made a difference.</p>
</div>
<table class="tableblock frame-all grid-all spread">
<caption class="title">Table 3. 99.9%ile latency end to end</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Batch Size</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">10 million events per minute</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">60 million events per minute</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">100 million events per minute</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">901 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">705 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">5,370 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">500 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1,610 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2,000 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">5</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">80 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1,540 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2,160 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">10</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">3,080 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1,470 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2,000 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">20</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">336 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1,210 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1,670 &micro;s</p></td>
</tr>
</tbody>
</table>
<table class="tableblock frame-all grid-all spread">
<caption class="title">Table 4. 99.9%ile latency to publish without replication</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Batch Size</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">10 million events per minute</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">60 million events per minute</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">100 million events per minute</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.2 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.3 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.5 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.2 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.3 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.5 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">5</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2.4 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.5 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.6 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">10</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">9.5 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.7 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">1.8 &micro;s</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">20</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">11 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">12 &micro;s</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">2.1 &micro;s</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>More investigation is required to draw any conclusions.</p>
</div>
</div>
        </div>

        <menu>
            <a href="" id="btn_share" class="btn" title="Share">
                <span aria-hidden="true" data-icon="S"></span>
                <strong>Share</strong>
            </a>
            <a href="http://twitter.com/share?text=Batching%20and%20Low%20Latency&url=https://vanilla-java.github.io/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" id="btn_comment" class="btn" target="_blank">
                <span aria-hidden="true" data-icon="C"></span> Comment on Twitter
            </a>
        </menu>


        <footer class="post-footer" role="contentinfo">

            <div class="vcard">
                <a href="https://vanilla-java.github.io/rss" id="btn_feed" class="btn" title="Feed" target="_blank">
                    <span aria-hidden="true" data-icon=")"></span>
                    <strong>Feed</strong>
                </a>

                <a href="https://vanilla-java.github.io/author/peter-lawrey/" class="photo">
                    <span style="background-image: url('https://avatars0.githubusercontent.com/u/1070321?v&#x3D;4');">
                        <img src="https://avatars0.githubusercontent.com/u/1070321?v&#x3D;4" alt="Peter Lawrey">
                    </span>
                </a>

                <div class="details">
                    <h4><a href="https://vanilla-java.github.io/author/peter-lawrey/" class="url n">Peter Lawrey</a></h4>
                    London<br>
                    <a href="http://vanillajava.blogspot.com/" class="js-remove-domain-schema">http://vanillajava.blogspot.com/</a>
                </div>
            </div>

            <div id="user_bio">
                <div class="inner">
                    Most answers for Java and JVM on StackOverflow.com (~13K), &quot;Vanilla Java&quot; blog with four million views, founder of the Performance JUG,  Java Champion
                </div>
            </div>

        </footer>




    <section class="post-comments">
      <div id="disqus_thread"></div>
      <script type="text/javascript">
      var disqus_shortname = 'vanillajava'; // required: replace example with your forum shortname
      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
      </script>
      <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
      <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </section>


    </article>

    <div id="share_modal">
        <div class="wrap">
            <div class="inner">
                <header>
                    Share
                    <a href="" class="close" title="Close">&times;</a>
                </header>

                <div class="roon-share-links">
                    <a href="https://twitter.com/share" class="twitter-share-button" data-dnt="true">Tweet</a>
                    <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>

                    <div id="fb-elems">
                        <div id="fb-root"></div>
                        <script>(function(d, s, id) {
                        var js, fjs = d.getElementsByTagName(s)[0];
                        if (d.getElementById(id)) return;
                        js = d.createElement(s); js.id = id;
                        js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=463438580397968";
                        fjs.parentNode.insertBefore(js, fjs);
                        }(document, 'script', 'facebook-jssdk'));</script>
                        <div class="fb-like" data-send="false" data-layout="button_count" data-width="110" data-show-faces="false" data-font="arial"></div>
                    </div>

                    <div id="pinit-btn">
                        <a href="//pinterest.com/pin/create/button/?url=https://vanilla-java.github.io/&amp;description=Batching%20and%20Low%20Latency-Vanilla%20Java " data-pin-do="buttonPin" data-pin-config="beside"><img src="//assets.pinterest.com/images/pidgets/pin_it_button.png"></a>
                        <script type="text/javascript" src="//assets.pinterest.com/js/pinit.js"></script>
                    </div>
                </div>
            </div>
        </div>
    </div>




        <a href="https://vanilla-java.github.io" id="blog_badge">
            <span style="background-image: url('https://raw.githubusercontent.com/Vanilla-Java/vanilla-java.github.io/master/images/French-Vanilla-Java.jpg');">Vanilla Java</span>
        </a>


    <script>

            function get_text(el) {
                ret = "";
                var length = el.childNodes.length;
                for(var i = 0; i < length; i++) {
                    var node = el.childNodes[i];
                    if(node.nodeType != 8) {
                        ret += node.nodeType != 1 ? node.nodeValue : get_text(node);
                    }
                }
                return ret;
            }
            function reading_time () {
                var post_content = document.getElementById('js-post-content');
                if (post_content) {
                    var words = get_text(post_content),
                        count = words.split(/\s+/).length,
                        read_time = Math.ceil((count / 150)),
                        read_time_node = document.createTextNode(read_time + ' min read');
                    document.getElementById('js-reading-time').appendChild(read_time_node);
                }
            }

        function no_schema_links () {
            var links = document.querySelectorAll('.js-remove-domain-schema');
            if (links) {
                for (i = 0; i < links.length; ++i) {
                    var link = links[i],
                        text = link.innerHTML,
                        no_schema = text.replace(/.*?:\/\//g, "");
                    link.innerHTML = no_schema;
                }
            }
        }

        window.onload = function () {
            no_schema_links();

            reading_time();
        }
    </script>


    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.9.0/moment-with-locales.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/prism.min.js?v="></script> 
      <script type="text/javascript">
        jQuery( document ).ready(function() {
          // change date with ago
          jQuery('ago.ago').each(function(){
            var element = jQuery(this).parent();
            element.html( moment(element.text()).fromNow());
          });
        });

        // hljs.initHighlightingOnLoad();
      </script>

        <script>
            $(function(){
                var share_modal = $("#share_modal"),
                    update_social_links = true;

                $("#btn_share").click(function(){
                    var that = $(this);
                    share_modal.fadeIn(200);
                    return false;
                });

                share_modal.click(function(e){
                    if (e.target.className == "wrap" || e.target.id == "share_modal") {
                        share_modal.fadeOut(200);
                    }
                    return false;
                });

                share_modal.find("div.inner > header > a.close").click(function(){
                    share_modal.fadeOut(200);
                    return false;
                });
            });
        </script>


    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-81039510-1', 'auto');
    ga('send', 'pageview');

    </script>

</body>
</html>
